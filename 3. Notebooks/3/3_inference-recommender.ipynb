{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Inference Recommender\n",
    "\n",
    "## Contents\n",
    "[1. Introduction](#1.-Introduction)  \n",
    "[2. Download the Model & payload](#2.-Download-the-Model-&-payload)  \n",
    "[3. Machine Learning model details](#3.-Machine-Learning-model-details)  \n",
    "[4. Register Model Version/Package](#4.-Register-Model-Version/Package)  \n",
    "[5. Create a SageMaker Inference Recommender Default Job](#5:-Create-a-SageMaker-Inference-Recommender-Default-Job)   \n",
    "[6. Instance Recommendation Results](#6.-Instance-Recommendation-Results)   \n",
    "[7. Create a SageMaker Inference Recommender Advanced Job](#7.-Custom-Load-Test)  \n",
    "[8. Describe result of an Advanced Job](#8.-Custom-Load-Test-Results)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "SageMaker Inference Recommender is a new capability of SageMaker that reduces the time required to get machine learning (ML) models in production by automating performance benchmarking and load testing models across SageMaker ML instances. You can use Inference Recommender to deploy your model to a real-time inference endpoint that delivers the best performance at the lowest cost. \n",
    "\n",
    "Get started with Inference Recommender on SageMaker in minutes while selecting an instance and get an optimized endpoint configuration in hours, eliminating weeks of manual testing and tuning time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, let's install the wheels of the required packages ie SageMaker Python SDK, boto3, botocore and awscli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /usr/local/lib/python3.7/site-packages (2.81.0)\n",
      "Requirement already satisfied: botocore in /usr/local/lib/python3.7/site-packages (1.24.27)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/site-packages (1.21.27)\n",
      "Requirement already satisfied: awscli in /usr/local/lib/python3.7/site-packages (1.22.82)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.7/site-packages (from awscli) (0.15.2)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/site-packages (from awscli) (0.5.2)\n",
      "Requirement already satisfied: colorama<0.4.4,>=0.2.5 in /usr/local/lib/python3.7/site-packages (from awscli) (0.4.3)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /usr/local/lib/python3.7/site-packages (from awscli) (4.7.2)\n",
      "Requirement already satisfied: PyYAML<5.5,>=3.10 in /usr/local/lib/python3.7/site-packages (from awscli) (5.4.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from botocore) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/site-packages (from botocore) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/site-packages (from botocore) (1.25.11)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.16.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/site-packages (from sagemaker) (20.9)\n",
      "Requirement already satisfied: attrs==20.3.0 in /usr/local/lib/python3.7/site-packages (from sagemaker) (20.3.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /usr/local/lib/python3.7/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: pathos in /usr/local/lib/python3.7/site-packages (from sagemaker) (0.2.8)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/site-packages (from sagemaker) (1.18.5)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /usr/local/lib/python3.7/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/site-packages (from sagemaker) (0.24.2)\n",
      "Requirement already satisfied: google-pasta in /usr/local/lib/python3.7/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.1 in /usr/local/lib/python3.7/site-packages (from sagemaker) (3.15.8)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /usr/local/lib/python3.7/site-packages (from sagemaker) (3.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=1.4.0->sagemaker) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=1.4.0->sagemaker) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.7/site-packages (from pandas->sagemaker) (2021.1)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /usr/local/lib/python3.7/site-packages (from pathos->sagemaker) (1.6.6.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /usr/local/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.0)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /usr/local/lib/python3.7/site-packages (from pathos->sagemaker) (0.70.12.2)\n",
      "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker botocore boto3 awscli --upgrade\n",
    "\n",
    "# !pip install sagemaker botocore boto3 awscli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download the Model & payload \n",
    "\n",
    "In this example, we are using Resnet50 Image Classification Model. Use Tensorflow 1.15 Python 3.7 kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sagemaker_session = Session()\n",
    "\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Directory  ./model/1  already exists\n",
      "INFO:tensorflow:Assets written to: ./model/1/assets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "tf.keras.backend.set_learning_phase(0)\n",
    "model = tf.keras.applications.ResNet50()\n",
    "\n",
    "# Creating the directory strcture\n",
    "model_version = \"1\"\n",
    "export_dir = \"./model/\" + model_version\n",
    "if not os.path.exists(export_dir):\n",
    "    os.makedirs(export_dir)\n",
    "    print(\"Directory \", export_dir, \" Created \")\n",
    "else:\n",
    "    print(\"Directory \", export_dir, \" already exists\")\n",
    "\n",
    "model_archive_name = \"model.tar.gz\"\n",
    "payload_archive_name = \"payload.tar.gz\"\n",
    "\n",
    "# Save to SavedModel\n",
    "model.save(export_dir, save_format=\"tf\", include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tar the model and code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/\n",
      "./model/1/\n",
      "./model/1/variables/\n",
      "./model/1/variables/variables.data-00001-of-00002\n",
      "./model/1/variables/variables.data-00000-of-00002\n",
      "./model/1/variables/variables.index\n",
      "./model/1/saved_model.pb\n",
      "./model/1/assets/\n",
      "./code/\n",
      "./code/requirements.txt\n",
      "./code/inference.py\n"
     ]
    }
   ],
   "source": [
    "!tar -cvpzf model.tar.gz ./model ./code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the payload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  ./sample-payload/  already exists\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "SAMPLES_BUCKET = \"sagemaker-sample-files\"\n",
    "PREFIX = \"datasets/image/pets/\"\n",
    "\n",
    "payload_location = \"./sample-payload/\"\n",
    "\n",
    "if not os.path.exists(payload_location):\n",
    "    os.makedirs(payload_location)\n",
    "    print(\"Directory \", payload_location, \" Created \")\n",
    "else:\n",
    "    print(\"Directory \", payload_location, \" already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2105k  100 2105k    0     0  19.2M      0 --:--:-- --:--:-- --:--:-- 19.2M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  245k  100  245k    0     0  3505k      0 --:--:-- --:--:-- --:--:-- 3505k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  321k  100  321k    0     0  3968k      0 --:--:-- --:--:-- --:--:-- 3968k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  562k  100  562k    0     0  5988k      0 --:--:-- --:--:-- --:--:-- 5988k\n"
     ]
    }
   ],
   "source": [
    "!curl  https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/pets/boxer_dog.jpg > ./sample-payload/boxer_dog.jpg\n",
    "!curl  https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/pets/british_blue_shorthair_cat.jpg > ./sample-payload/british_blue_shorthair_cat.jpg\n",
    "!curl  https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/pets/english_cocker_spaniel_dog.jpg > ./sample-payload/english_cocker_spaniel_dog.jpg\n",
    "!curl  https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/pets/shiba_inu_dog.jpg > ./sample-payload/shiba_inu_dog.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tar the payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxer_dog.jpg\n",
      "british_blue_shorthair_cat.jpg\n",
      "english_cocker_spaniel_dog.jpg\n",
      "shiba_inu_dog.jpg\n"
     ]
    }
   ],
   "source": [
    "!cd ./sample-payload/ && tar czvf ../payload.tar.gz *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to S3\n",
    "\n",
    "We now have a model archive ready. We need to upload it to S3 before we can use it with Inference Recommender. We'll use the SageMaker Python SDK to handle the upload.\n",
    "\n",
    "We need to create an archive that contains individual files that Inference Recommender can send to your SageMaker Endpoints. Inference Recommender will randomly sample files from this archive so make sure it contains a similar distribution of payloads you'd expect in production. Note that your inference code must be able to read in the file formats from the sample payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-871059659476/sagemaker/inference-recommender/inference/payload.tar.gz\n",
      "s3://sagemaker-us-east-1-871059659476/sagemaker/inference-recommender/reset50/model/model.tar.gz\n",
      "CPU times: user 835 ms, sys: 180 ms, total: 1.02 s\n",
      "Wall time: 1.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket and prefix\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "\n",
    "prefix = \"sagemaker/inference-recommender\"\n",
    "\n",
    "sample_payload_url = sagemaker.Session().upload_data(\n",
    "    payload_archive_name, bucket=bucket, key_prefix=prefix + \"/inference\"\n",
    ")\n",
    "model_url = sagemaker.Session().upload_data(\n",
    "    model_archive_name, bucket=bucket, key_prefix=prefix + \"/reset50/model\"\n",
    ")\n",
    "\n",
    "print(sample_payload_url)\n",
    "print(model_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Machine Learning model details\n",
    "\n",
    "Inference Recommender uses information about your ML model to recommend the best instance types and endpoint configurations for deployment. You can provide as much or as little information as you'd like and Inference Recommender will use that to provide recommendations.\n",
    "\n",
    "Example ML Domains: `COMPUTER_VISION`, `NATURAL_LANGUAGE_PROCESSING`, `MACHINE_LEARNING`\n",
    "\n",
    "Example ML Tasks: `CLASSIFICATION`, `REGRESSION`, `OBJECT_DETECTION`, `OTHER`\n",
    "\n",
    "Example Model name: `resnet50`, `yolov4`, `xgboost` etc\n",
    "\n",
    "Use list_model_metadata API to fetch the list of available models. This will help you to pick the closest model for better recommendation. In this example, as we pick the `resnet50` model we selected `COMPUTER_VISION` as the Domain, `IMAGE_CLASSIFICATION` as the Task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Domain</th>\n",
       "      <th>Task</th>\n",
       "      <th>Framework</th>\n",
       "      <th>FrameworkVersion</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_CLASSIFICATION</td>\n",
       "      <td>MXNET</td>\n",
       "      <td>1.8.0</td>\n",
       "      <td>densenet201-gluon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_CLASSIFICATION</td>\n",
       "      <td>MXNET</td>\n",
       "      <td>1.8.0</td>\n",
       "      <td>resnet18v2-gluon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_CLASSIFICATION</td>\n",
       "      <td>PYTORCH</td>\n",
       "      <td>1.6.0</td>\n",
       "      <td>resnet152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_CLASSIFICATION</td>\n",
       "      <td>TENSORFLOW</td>\n",
       "      <td>1.15.5</td>\n",
       "      <td>efficientnetb7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_CLASSIFICATION</td>\n",
       "      <td>TENSORFLOW</td>\n",
       "      <td>1.15.5</td>\n",
       "      <td>nasnetlarge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_CLASSIFICATION</td>\n",
       "      <td>TENSORFLOW</td>\n",
       "      <td>1.15.5</td>\n",
       "      <td>vgg16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_CLASSIFICATION</td>\n",
       "      <td>TENSORFLOW</td>\n",
       "      <td>1.15.5</td>\n",
       "      <td>inception-v3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_CLASSIFICATION</td>\n",
       "      <td>TENSORFLOW</td>\n",
       "      <td>1.15.5</td>\n",
       "      <td>xception</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_CLASSIFICATION</td>\n",
       "      <td>TENSORFLOW</td>\n",
       "      <td>1.15.5</td>\n",
       "      <td>densenet201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_CLASSIFICATION</td>\n",
       "      <td>TENSORFLOW</td>\n",
       "      <td>1.15.5</td>\n",
       "      <td>xceptionV1-keras</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_CLASSIFICATION</td>\n",
       "      <td>TENSORFLOW</td>\n",
       "      <td>1.15.5</td>\n",
       "      <td>resnet50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_SEGMENTATION</td>\n",
       "      <td>PYTORCH</td>\n",
       "      <td>1.6.0</td>\n",
       "      <td>unet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_SEGMENTATION</td>\n",
       "      <td>PYTORCH</td>\n",
       "      <td>1.6.0</td>\n",
       "      <td>mask-rcnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>OBJECT_DETECTION</td>\n",
       "      <td>PYTORCH</td>\n",
       "      <td>1.6.0</td>\n",
       "      <td>yolov4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>OBJECT_DETECTION</td>\n",
       "      <td>TENSORFLOW</td>\n",
       "      <td>1.15.5</td>\n",
       "      <td>faster-rcnn-resnet101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>OBJECT_DETECTION</td>\n",
       "      <td>TENSORFLOW</td>\n",
       "      <td>1.15.5</td>\n",
       "      <td>retinanet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MACHINE_LEARNING</td>\n",
       "      <td>CLASSIFICATION</td>\n",
       "      <td>XGBOOST</td>\n",
       "      <td>1.0-1</td>\n",
       "      <td>xgboost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MACHINE_LEARNING</td>\n",
       "      <td>REGRESSION</td>\n",
       "      <td>SAGEMAKER-SCIKIT-LEARN</td>\n",
       "      <td>0.23-1</td>\n",
       "      <td>sagemaker-scikit-learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MACHINE_LEARNING</td>\n",
       "      <td>REGRESSION</td>\n",
       "      <td>XGBOOST</td>\n",
       "      <td>1.3-1</td>\n",
       "      <td>xgboost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NATURAL_LANGUAGE_PROCESSING</td>\n",
       "      <td>FILL_MASK</td>\n",
       "      <td>PYTORCH</td>\n",
       "      <td>1.6.0</td>\n",
       "      <td>bert-base-cased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NATURAL_LANGUAGE_PROCESSING</td>\n",
       "      <td>FILL_MASK</td>\n",
       "      <td>PYTORCH</td>\n",
       "      <td>1.6.0</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Domain                     Task                Framework        FrameworkVersion          Model         \n",
       "9               COMPUTER_VISION  IMAGE_CLASSIFICATION                   MXNET       1.8.0            densenet201-gluon\n",
       "10              COMPUTER_VISION  IMAGE_CLASSIFICATION                   MXNET       1.8.0             resnet18v2-gluon\n",
       "14              COMPUTER_VISION  IMAGE_CLASSIFICATION                 PYTORCH       1.6.0                    resnet152\n",
       "0               COMPUTER_VISION  IMAGE_CLASSIFICATION              TENSORFLOW      1.15.5               efficientnetb7\n",
       "4               COMPUTER_VISION  IMAGE_CLASSIFICATION              TENSORFLOW      1.15.5                  nasnetlarge\n",
       "5               COMPUTER_VISION  IMAGE_CLASSIFICATION              TENSORFLOW      1.15.5                        vgg16\n",
       "6               COMPUTER_VISION  IMAGE_CLASSIFICATION              TENSORFLOW      1.15.5                 inception-v3\n",
       "11              COMPUTER_VISION  IMAGE_CLASSIFICATION              TENSORFLOW      1.15.5                     xception\n",
       "12              COMPUTER_VISION  IMAGE_CLASSIFICATION              TENSORFLOW      1.15.5                  densenet201\n",
       "17              COMPUTER_VISION  IMAGE_CLASSIFICATION              TENSORFLOW      1.15.5             xceptionV1-keras\n",
       "18              COMPUTER_VISION  IMAGE_CLASSIFICATION              TENSORFLOW      1.15.5                     resnet50\n",
       "1               COMPUTER_VISION    IMAGE_SEGMENTATION                 PYTORCH       1.6.0                         unet\n",
       "7               COMPUTER_VISION    IMAGE_SEGMENTATION                 PYTORCH       1.6.0                    mask-rcnn\n",
       "13              COMPUTER_VISION      OBJECT_DETECTION                 PYTORCH       1.6.0                       yolov4\n",
       "3               COMPUTER_VISION      OBJECT_DETECTION              TENSORFLOW      1.15.5        faster-rcnn-resnet101\n",
       "19              COMPUTER_VISION      OBJECT_DETECTION              TENSORFLOW      1.15.5                    retinanet\n",
       "2              MACHINE_LEARNING        CLASSIFICATION                 XGBOOST       1.0-1                      xgboost\n",
       "8              MACHINE_LEARNING            REGRESSION  SAGEMAKER-SCIKIT-LEARN      0.23-1       sagemaker-scikit-learn\n",
       "15             MACHINE_LEARNING            REGRESSION                 XGBOOST       1.3-1                      xgboost\n",
       "16  NATURAL_LANGUAGE_PROCESSING             FILL_MASK                 PYTORCH       1.6.0              bert-base-cased\n",
       "20  NATURAL_LANGUAGE_PROCESSING             FILL_MASK                 PYTORCH       1.6.0            bert-base-uncased"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "inference_client = boto3.client(\"sagemaker\", region)\n",
    "\n",
    "list_model_metadata_response = inference_client.list_model_metadata()\n",
    "\n",
    "domains = []\n",
    "frameworks = []\n",
    "framework_versions = []\n",
    "tasks = []\n",
    "models = []\n",
    "\n",
    "for model_summary in list_model_metadata_response[\"ModelMetadataSummaries\"]:\n",
    "    domains.append(model_summary[\"Domain\"])\n",
    "    tasks.append(model_summary[\"Task\"])\n",
    "    models.append(model_summary[\"Model\"])\n",
    "    frameworks.append(model_summary[\"Framework\"])\n",
    "    framework_versions.append(model_summary[\"FrameworkVersion\"])\n",
    "\n",
    "data = {\n",
    "    \"Domain\": domains,\n",
    "    \"Task\": tasks,\n",
    "    \"Framework\": frameworks,\n",
    "    \"FrameworkVersion\": framework_versions,\n",
    "    \"Model\": models,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "pd.set_option(\"display.colheader_justify\", \"center\")\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "\n",
    "\n",
    "display(df.sort_values(by=[\"Domain\", \"Task\", \"Framework\", \"FrameworkVersion\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container image URL\n",
    "\n",
    "If you don’t have an inference container image, you can use one of the open source [deep learning containers (DLCs)](https://github.com/aws/deep-learning-containers) provided by AWS to serve your ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:1.15.4-cpu'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker import image_uris\n",
    "\n",
    "instance_type = \"ml.c5.xlarge\"  # Note: you can use any CPU-based instance type here, this is just to get CPU tagged image\n",
    "# Adding framework related parameters\n",
    "framework_name = \"tensorflow\"\n",
    "framework_version = \"1.15.4\"\n",
    "\n",
    "# ML model details\n",
    "ml_domain = \"COMPUTER_VISION\"\n",
    "ml_task = \"IMAGE_CLASSIFICATION\"\n",
    "model_name = \"resnet50\"\n",
    "\n",
    "dlc_uri = image_uris.retrieve(\n",
    "    framework_name,\n",
    "    region,\n",
    "    version=framework_version,\n",
    "    py_version=\"py3\",\n",
    "    instance_type=instance_type,\n",
    "    image_scope=\"inference\",\n",
    ")\n",
    "dlc_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Register Model Version/Package\n",
    "\n",
    "Inference Recommender expects the model to be packaged in the model registry. Here, we are creating a model package group and a model package version. The model package version which takes container, model url etc, will now allow you to pass additional information about the model like `Domain`, `Task`, `Framework`, `FrameworkVersion`, `NearestModelName`, `SamplePayloadUrl`\n",
    "\n",
    "As `SamplePayloadUrl` and `SupportedContentTypes` parameters are essential for benchmarking the endpoint. We also highly recommend you to specific `Domain`, `Task`, `Framework`, `FrameworkVersion`, `NearestModelName` for better inference recommendation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d7660b10-aeaf-11ec-9edc-aefbac57ee16\n",
      "{'ModelPackageGroupArn': 'arn:aws:sagemaker:us-east-1:871059659476:model-package-group/d7660b10-aeaf-11ec-9edc-aefbac57ee16', 'ResponseMetadata': {'RequestId': 'f56fcf1f-b543-46da-8c83-e0a9364bf689', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'f56fcf1f-b543-46da-8c83-e0a9364bf689', 'content-type': 'application/x-amz-json-1.1', 'content-length': '124', 'date': 'Mon, 28 Mar 2022 15:57:59 GMT'}, 'RetryAttempts': 0}}\n",
      "{'ModelPackageArn': 'arn:aws:sagemaker:us-east-1:871059659476:model-package/d7660b10-aeaf-11ec-9edc-aefbac57ee16/1', 'ResponseMetadata': {'RequestId': 'a54ff00b-abea-4e0f-bcbb-9d2a37abfe88', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'a54ff00b-abea-4e0f-bcbb-9d2a37abfe88', 'content-type': 'application/x-amz-json-1.1', 'content-length': '115', 'date': 'Mon, 28 Mar 2022 15:57:59 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import uuid\n",
    "\n",
    "inference_client = boto3.client(\"sagemaker\", region)\n",
    "\n",
    "model_package_group_name = uuid.uuid1()\n",
    "print(model_package_group_name)\n",
    "model_pacakge_group_response = inference_client.create_model_package_group(\n",
    "    ModelPackageGroupName=str(model_package_group_name), ModelPackageGroupDescription=\"description\"\n",
    ")\n",
    "\n",
    "print(model_pacakge_group_response)\n",
    "\n",
    "model_package_version_response = inference_client.create_model_package(\n",
    "    ModelPackageGroupName=str(model_package_group_name),\n",
    "    ModelPackageDescription=\"InferenceRecommenderDemo\",\n",
    "    Domain=ml_domain,\n",
    "    Task=ml_task,\n",
    "    SamplePayloadUrl=sample_payload_url,\n",
    "    InferenceSpecification={\n",
    "        \"Containers\": [\n",
    "            {\n",
    "                \"ContainerHostname\": \"dlc\",\n",
    "                \"Image\": dlc_uri,\n",
    "                \"ModelDataUrl\": model_url,\n",
    "                \"Framework\": \"TENSORFLOW\",\n",
    "                \"FrameworkVersion\": framework_version,\n",
    "                \"NearestModelName\": model_name,\n",
    "                \"ModelInput\": {\"DataInputConfig\": '{\"input_1\":[1,3,224,224]}'},\n",
    "            },\n",
    "        ],\n",
    "        \"SupportedRealtimeInferenceInstanceTypes\": [\n",
    "            \"ml.c5.xlarge\",\n",
    "            \"ml.c5.2xlarge\",\n",
    "            \"ml.m5.xlarge\",\n",
    "            \"ml.m5.2xlarge\",\n",
    "            \"ml.m5.4xlarge\",\n",
    "            \"ml.r5.large\",\n",
    "            \"ml.r5.xlarge\",\n",
    "            \"ml.r5.2xlarge\",\n",
    "            \"ml.r5.4xlarge\",\n",
    "            \"ml.r5.12xlarge\",\n",
    "            \"ml.r5.24xlarge\",\n",
    "            \"ml.r5d.large\",\n",
    "            \"ml.r5d.xlarge\",\n",
    "            \"ml.r5d.2xlarge\",\n",
    "            \"ml.r5d.4xlarge\",\n",
    "            \"ml.r5d.12xlarge\",\n",
    "            \"ml.r5d.24xlarge\",\n",
    "            \"ml.inf1.xlarge\",\n",
    "            \"ml.inf1.2xlarge\",\n",
    "            \"ml.inf1.6xlarge\",\n",
    "            \"ml.inf1.24xlarge\",\n",
    "        ],\n",
    "        \"SupportedContentTypes\": [\n",
    "            \"application/x-image\",\n",
    "        ],\n",
    "        \"SupportedResponseMIMETypes\": [],\n",
    "    },\n",
    ")\n",
    "\n",
    "print(model_package_version_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Create a SageMaker Inference Recommender Default Job\n",
    "\n",
    "Now with your model in Model Registry, you can kick off a 'Default' job to get instance recommendations. This only requires your `ModelPackageVersionArn` and comes back with recommendations within an hour. \n",
    "\n",
    "The output is a list of instance type recommendations with associated environment variables, cost, throughput and latency metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JobArn': 'arn:aws:sagemaker:us-east-1:871059659476:inference-recommendations-job/d7a977c4-aeaf-11ec-9edc-aefbac57ee16', 'ResponseMetadata': {'RequestId': '0d0d390e-7029-406b-87f9-05ad661997b6', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '0d0d390e-7029-406b-87f9-05ad661997b6', 'content-type': 'application/x-amz-json-1.1', 'content-length': '120', 'date': 'Mon, 28 Mar 2022 15:58:00 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import uuid\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "client = boto3.client(\"sagemaker\", region)\n",
    "\n",
    "role = get_execution_role()\n",
    "default_job = uuid.uuid1()\n",
    "default_response = client.create_inference_recommendations_job(\n",
    "    JobName=str(default_job),\n",
    "    JobDescription=\"Job Description\",\n",
    "    JobType=\"Default\",\n",
    "    RoleArn=role,\n",
    "    InputConfig={\"ModelPackageVersionArn\": model_package_version_response[\"ModelPackageArn\"]},\n",
    ")\n",
    "\n",
    "print(default_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Instance Recommendation Results\n",
    "\n",
    "Inference recommender job provides multiple endpoint recommendations in its result. The recommendation includes `InstanceType`, `InitialInstanceCount`, `EnvironmentParameters` which includes tuned parameters for better performance. We also include the benchmarking results like `MaxInvocations`, `ModelLatency`, `CostPerHour` and `CostPerInference` for deeper analysis. We believe the information provided  will help you narrow down to a specific endpoint configuration that suits your use case. \n",
    "\n",
    "Example:   \n",
    "\n",
    "If your motivation is overall price-performance, then you should focus on `CostPerInference` metrics  \n",
    "If your motivation is latency/throughput, then you should focus on `ModelLatency` / `MaxInvocations` metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infernece recommender job in progress\n",
      "Infernece recommender job in progress\n",
      "Infernece recommender job in progress\n",
      "Infernece recommender job in progress\n",
      "Infernece recommender job in progress\n",
      "Infernece recommender job in progress\n",
      "Infernece recommender job completed\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import uuid\n",
    "import pprint\n",
    "import pandas as pd\n",
    "\n",
    "client = boto3.client(\"sagemaker\", region)\n",
    "\n",
    "stopped = False\n",
    "while not stopped:\n",
    "    inference_recommender_job = client.describe_inference_recommendations_job(\n",
    "        JobName=str(default_job)\n",
    "    )\n",
    "    if inference_recommender_job[\"Status\"] in [\"COMPLETED\", \"STOPPED\", \"FAILED\"]:\n",
    "        stopped = True\n",
    "    else:\n",
    "        print(\"Infernece recommender job in progress\")\n",
    "        time.sleep(300)\n",
    "\n",
    "if inference_recommender_job[\"Status\"] == \"FAILED\":\n",
    "    print(\"Infernece recommender job failed \")\n",
    "    print(\"Failed Reason: {}\".inference_recommender_job[\"FailedReason\"])\n",
    "else:\n",
    "    print(\"Infernece recommender job completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailing out the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>CostPerHour</th>\n",
       "      <th>CostPerInference</th>\n",
       "      <th>EndpointName</th>\n",
       "      <th>EnvironmentParameters</th>\n",
       "      <th>InitialInstanceCount</th>\n",
       "      <th>InstanceType</th>\n",
       "      <th>MaxInvocations</th>\n",
       "      <th>ModelLatency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.922</td>\n",
       "      <td>2.794e-05</td>\n",
       "      <td>sm-epc-5cc60352-8e8b-4fb3-9a52-ad79b0479f93</td>\n",
       "      <td>[{'Key': 'TS_DEFAULT_WORKERS_PER_MODEL', 'ValueType': 'string', 'Value': '40'}, {'Key': 'OMP_NUM_THREADS', 'ValueType': 'string', 'Value': '1'}]</td>\n",
       "      <td>1</td>\n",
       "      <td>ml.m5.4xlarge</td>\n",
       "      <td>550</td>\n",
       "      <td>1057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.408</td>\n",
       "      <td>1.001e-05</td>\n",
       "      <td>sm-epc-75196af1-6f83-4b8c-aeb0-3ab06f188eff</td>\n",
       "      <td>[{'Key': 'TS_DEFAULT_WORKERS_PER_MODEL', 'ValueType': 'string', 'Value': '2'}, {'Key': 'OMP_NUM_THREADS', 'ValueType': 'string', 'Value': '1'}]</td>\n",
       "      <td>1</td>\n",
       "      <td>ml.c5.2xlarge</td>\n",
       "      <td>679</td>\n",
       "      <td>1149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.204</td>\n",
       "      <td>5.714e-06</td>\n",
       "      <td>sm-epc-e46c6180-c483-4dee-890a-3dc95eb4c1c3</td>\n",
       "      <td>[{'Key': 'TS_DEFAULT_WORKERS_PER_MODEL', 'ValueType': 'string', 'Value': '1'}, {'Key': 'OMP_NUM_THREADS', 'ValueType': 'string', 'Value': '1'}]</td>\n",
       "      <td>1</td>\n",
       "      <td>ml.c5.xlarge</td>\n",
       "      <td>595</td>\n",
       "      <td>2102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.230</td>\n",
       "      <td>8.653e-06</td>\n",
       "      <td>sm-epc-844fd66d-5862-4bc2-8089-046537aa6af6</td>\n",
       "      <td>[{'Key': 'TS_DEFAULT_WORKERS_PER_MODEL', 'ValueType': 'string', 'Value': '32'}, {'Key': 'OMP_NUM_THREADS', 'ValueType': 'string', 'Value': '1'}]</td>\n",
       "      <td>1</td>\n",
       "      <td>ml.m5.xlarge</td>\n",
       "      <td>443</td>\n",
       "      <td>3387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.461</td>\n",
       "      <td>1.543e-05</td>\n",
       "      <td>sm-epc-425c8a53-8efc-4fc0-a897-a61059609f45</td>\n",
       "      <td>[{'Key': 'TS_DEFAULT_WORKERS_PER_MODEL', 'ValueType': 'string', 'Value': '2'}, {'Key': 'OMP_NUM_THREADS', 'ValueType': 'string', 'Value': '1'}]</td>\n",
       "      <td>1</td>\n",
       "      <td>ml.m5.2xlarge</td>\n",
       "      <td>498</td>\n",
       "      <td>1092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CostPerHour  CostPerInference                 EndpointName                                                                               EnvironmentParameters                                                                InitialInstanceCount  InstanceType   MaxInvocations  ModelLatency\n",
       "0     0.922         2.794e-05     sm-epc-5cc60352-8e8b-4fb3-9a52-ad79b0479f93  [{'Key': 'TS_DEFAULT_WORKERS_PER_MODEL', 'ValueType': 'string', 'Value': '40'}, {'Key': 'OMP_NUM_THREADS', 'ValueType': 'string', 'Value': '1'}]            1           ml.m5.4xlarge        550           1057    \n",
       "1     0.408         1.001e-05     sm-epc-75196af1-6f83-4b8c-aeb0-3ab06f188eff   [{'Key': 'TS_DEFAULT_WORKERS_PER_MODEL', 'ValueType': 'string', 'Value': '2'}, {'Key': 'OMP_NUM_THREADS', 'ValueType': 'string', 'Value': '1'}]            1           ml.c5.2xlarge        679           1149    \n",
       "2     0.204         5.714e-06     sm-epc-e46c6180-c483-4dee-890a-3dc95eb4c1c3   [{'Key': 'TS_DEFAULT_WORKERS_PER_MODEL', 'ValueType': 'string', 'Value': '1'}, {'Key': 'OMP_NUM_THREADS', 'ValueType': 'string', 'Value': '1'}]            1            ml.c5.xlarge        595           2102    \n",
       "3     0.230         8.653e-06     sm-epc-844fd66d-5862-4bc2-8089-046537aa6af6  [{'Key': 'TS_DEFAULT_WORKERS_PER_MODEL', 'ValueType': 'string', 'Value': '32'}, {'Key': 'OMP_NUM_THREADS', 'ValueType': 'string', 'Value': '1'}]            1            ml.m5.xlarge        443           3387    \n",
       "4     0.461         1.543e-05     sm-epc-425c8a53-8efc-4fc0-a897-a61059609f45   [{'Key': 'TS_DEFAULT_WORKERS_PER_MODEL', 'ValueType': 'string', 'Value': '2'}, {'Key': 'OMP_NUM_THREADS', 'ValueType': 'string', 'Value': '1'}]            1           ml.m5.2xlarge        498           1092    "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    {**x[\"EndpointConfiguration\"], **x[\"ModelConfiguration\"], **x[\"Metrics\"]}\n",
    "    for x in inference_recommender_job[\"InferenceRecommendations\"]\n",
    "]\n",
    "df = pd.DataFrame(data)\n",
    "df.drop(\"VariantName\", inplace=True, axis=1)\n",
    "pd.set_option(\"max_colwidth\", 400)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Load Test\n",
    "\n",
    "With an 'Advanced' job, you can provide your production requirements, select instance types, tune environment variables and perform more extensive load tests. This typically takes 2 hours depending on your traffic pattern and number of instance types. \n",
    "\n",
    "The output is a list of endpoint configuration recommendations (instance type, instance count, environment variables) with associated cost, throughput and latency metrics.\n",
    "\n",
    "In the below example, we are tuning the endpoint against an environment variable `OMP_NUM_THREADS` with two values `[2, 4]` and we aim to limit the  latency requirement to `100` ms. The goal is to find which endpoint configuration provides the best performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JobArn': 'arn:aws:sagemaker:us-east-1:871059659476:inference-recommendations-job/0a29a1f2-aeb4-11ec-9edc-aefbac57ee16', 'ResponseMetadata': {'RequestId': 'ef718f40-dbfe-47b3-a723-3a18ff1b16b3', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'ef718f40-dbfe-47b3-a723-3a18ff1b16b3', 'content-type': 'application/x-amz-json-1.1', 'content-length': '120', 'date': 'Mon, 28 Mar 2022 16:28:02 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import uuid\n",
    "\n",
    "client = boto3.client(\"sagemaker\", region)\n",
    "\n",
    "role = get_execution_role()\n",
    "advanced_job = uuid.uuid1()\n",
    "advanced_response = client.create_inference_recommendations_job(\n",
    "    JobName=str(advanced_job),\n",
    "    JobDescription=\"JobDescription\",\n",
    "    JobType=\"Advanced\",\n",
    "    RoleArn=role,\n",
    "    InputConfig={\n",
    "        \"ModelPackageVersionArn\": model_package_version_response[\"ModelPackageArn\"],\n",
    "        \"JobDurationInSeconds\": 7200,\n",
    "        \"EndpointConfigurations\": [\n",
    "            {\n",
    "                \"InstanceType\": \"ml.m5.xlarge\",\n",
    "                \"EnvironmentParameterRanges\": {\n",
    "                    \"CategoricalParameterRanges\": [{\"Name\": \"OMP_NUM_THREADS\", \"Value\": [\"2\", \"4\"]}]\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        \"ResourceLimit\": {\"MaxNumberOfTests\": 2, \"MaxParallelOfTests\": 1},\n",
    "        \"TrafficPattern\": {\n",
    "            \"TrafficType\": \"PHASES\",\n",
    "            \"Phases\": [{\"InitialNumberOfUsers\": 1, \"SpawnRate\": 1, \"DurationInSeconds\": 120}],\n",
    "        },\n",
    "    },\n",
    "    StoppingConditions={\n",
    "        \"MaxInvocations\": 300,\n",
    "        \"ModelLatencyThresholds\": [{\"Percentile\": \"P95\", \"ValueInMilliseconds\": 100}],\n",
    "    },\n",
    ")\n",
    "\n",
    "print(advanced_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Custom Load Test Results\n",
    "\n",
    "Inference recommender does benchmarking on both the endpoint configurations and here is the result. \n",
    "\n",
    "Analyzing load test result,    \n",
    "`OMP_NUM_THREADS` = 2 shows ~20% better throughput when compared to `OMP_NUM_THREADS` = 4   \n",
    "`OMP_NUM_THREADS` = 2 shows 25% saving in inference-cost when compared to `OMP_NUM_THREADS` = 4   \n",
    "\n",
    "In all front, `OMP_NUM_THREADS` = 2  is much better endpoint configuration than `OMP_NUM_THREADS` = 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infernece recommender job in progress\n",
      "Infernece recommender job in progress\n",
      "Infernece recommender job in progress\n",
      "Infernece recommender job in progress\n",
      "Infernece recommender job in progress\n",
      "Infernece recommender job in progress\n",
      "Infernece recommender job completed\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import uuid\n",
    "import pprint\n",
    "import pandas as pd\n",
    "\n",
    "client = boto3.client(\"sagemaker\", region)\n",
    "\n",
    "stopped = False\n",
    "while not stopped:\n",
    "    inference_recommender_job = client.describe_inference_recommendations_job(\n",
    "        JobName=str(advanced_job)\n",
    "    )\n",
    "    if inference_recommender_job[\"Status\"] in [\"COMPLETED\", \"STOPPED\", \"FAILED\"]:\n",
    "        stopped = True\n",
    "    else:\n",
    "        print(\"Infernece recommender job in progress\")\n",
    "        time.sleep(300)\n",
    "\n",
    "if inference_recommender_job[\"Status\"] == \"FAILED\":\n",
    "    print(\"Infernece recommender job failed \")\n",
    "    print(\"Failed Reason: {}\".inference_recommender_job[\"FailedReason\"])\n",
    "else:\n",
    "    print(\"Infernece recommender job completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailing out the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>CostPerHour</th>\n",
       "      <th>CostPerInference</th>\n",
       "      <th>EndpointName</th>\n",
       "      <th>EnvironmentParameters</th>\n",
       "      <th>InitialInstanceCount</th>\n",
       "      <th>InstanceType</th>\n",
       "      <th>MaxInvocations</th>\n",
       "      <th>ModelLatency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.46</td>\n",
       "      <td>2.190e-05</td>\n",
       "      <td>sm-epc-e829c66f-8fa6-4d77-8247-156af36a063d</td>\n",
       "      <td>[{'Key': 'OMP_NUM_THREADS', 'ValueType': 'string', 'Value': '2'}]</td>\n",
       "      <td>2</td>\n",
       "      <td>ml.m5.xlarge</td>\n",
       "      <td>350</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.69</td>\n",
       "      <td>3.453e-05</td>\n",
       "      <td>sm-epc-ba4a6b24-35f2-44ea-ac6f-5c7c2ed1fe3a</td>\n",
       "      <td>[{'Key': 'OMP_NUM_THREADS', 'ValueType': 'string', 'Value': '4'}]</td>\n",
       "      <td>3</td>\n",
       "      <td>ml.m5.xlarge</td>\n",
       "      <td>333</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CostPerHour  CostPerInference                 EndpointName                                       EnvironmentParameters                         InitialInstanceCount  InstanceType  MaxInvocations  ModelLatency\n",
       "0     0.46          2.190e-05     sm-epc-e829c66f-8fa6-4d77-8247-156af36a063d  [{'Key': 'OMP_NUM_THREADS', 'ValueType': 'string', 'Value': '2'}]            2           ml.m5.xlarge        350            100    \n",
       "1     0.69          3.453e-05     sm-epc-ba4a6b24-35f2-44ea-ac6f-5c7c2ed1fe3a  [{'Key': 'OMP_NUM_THREADS', 'ValueType': 'string', 'Value': '4'}]            3           ml.m5.xlarge        333            100    "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    {**x[\"EndpointConfiguration\"], **x[\"ModelConfiguration\"], **x[\"Metrics\"]}\n",
    "    for x in inference_recommender_job[\"InferenceRecommendations\"]\n",
    "]\n",
    "df = pd.DataFrame(data)\n",
    "df.drop(\"VariantName\", inplace=True, axis=1)\n",
    "pd.set_option(\"max_colwidth\", 400)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 1.15 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-1.15-cpu-py37-ubuntu18.04-v7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
